# Llama Copilot Proxy environment configuration
# Copy this file to .env and adjust values as needed.

# Host and port to bind the proxy
LISTEN_HOST=0.0.0.0
LISTEN_PORT=11434

# Upstream OpenAI-compatible base URL (e.g., llama.cpp server or OpenAI proxy)
# Example: http://127.0.0.1:8080
UPSTREAM=http://127.0.0.1:8080

# Thinking/Reasoning display mode: default | events | both | show_reasoning | off
THINKING_MODE=default

# Enable extra debug logs for thinking stream processing
THINKING_DEBUG=false

# Verbose request/response logging
VERBOSE=false
