
version: '3.8'
services:
  proxy:
    build:
      context: .
      target: prod
    ports:
      - "11434:11434"
    environment:
      - VERBOSE=1
      - LLAMA_SERVER_PORT=8080 # Make llama-server port configurable
      - THINKING_MODE=show_reasoning   # Route thinking to normal content stream (VSCode will display it!)
      - THINKING_DEBUG=true     # Enable debug mode for thinking events (optional)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/health"]
      interval: 10s
      timeout: 5s
      retries: 5
